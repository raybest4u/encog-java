#define POSITIVE_ETA 1.2f
#define NEGATIVE_ETA 0.5f
#define DELTA_MIN 0.00001f
#define MAX_STEP 50.0f	

kernel void NetworkLearnRPROP(
    global read_only int *params,
    global write_only float *errors,
    global read_only int *layerIndex,
    global read_only int *layerCounts,
    global read_only int *layerFeedCounts,
    global read_only int *weightIndex,
    global read_only float* input,
    global read_only float* ideal,
    global read_only float* weightsIn,
    global write_only float* weightsOut,
    global write_only float *gradients,
    global read_only int *activationType,
    global read_only float *slope,
    global read_only float *tempDataIn,
    global read_only float *tempDataOut
    )
{	
	// now that the gradients have been calculated, update the network

	// loop over all gradients and sum them into the first subtask
	for(int i=0;i<WEIGHT_COUNT;i++) 
	{
		for(int j=1;j<=indexLastElement;j++)
		{		 
			gradients[i] += gradients[(j*WEIGHT_COUNT)+i];
		}
	}

	// teach the weights
	
	global float *wptr = weightsIn;
	global float *gptr = gradients;		
	for(int i=0;i<WEIGHT_COUNT;i++)
	{
		int change = sign((*gptr) * tempDataIn[i]);
		float weightChange = 0;

		// if the gradient has retained its sign, then we increase the
		// delta so that it will converge faster
		if (change > 0) 
		{
			float delta = tempDataIn[i+WEIGHT_COUNT]
				* POSITIVE_ETA;
			delta = min(delta, MAX_STEP);
			weightChange = sign(*gptr) * delta;
			tempDataIn[i+WEIGHT_COUNT] = delta;
			tempDataIn[i] = *gptr;
			}
		else if (change < 0) 
		{
			// if change<0, then the sign has changed, and the last
			// delta was too big
			float delta = tempDataIn[i+WEIGHT_COUNT]
				* NEGATIVE_ETA;
			delta = max(delta, DELTA_MIN);
			tempDataIn[i+WEIGHT_COUNT] = delta;
			// set the previous gradient to zero so that there will be no
			// adjustment the next iteration
			tempDataIn[i] = 0;
		} 
		else if (change == 0) 
		{
			// if change==0 then there is no change to the delta
			float delta = tempDataIn[i+WEIGHT_COUNT];
			weightChange = sign(*gptr) * delta;
			tempDataIn[i] = gradients[i];
		}
			
		*(wptr++)+=weightChange;
		gptr++;
	}	
	
	// finally, after all is done, return the weights to the CPU
	for(int i=0;i<WEIGHT_COUNT;i++)
	{
		weightsOut[i] = weightsIn[i];
	}
		
	for(int i=0;i<(WEIGHT_COUNT*2);i++)
	{
		tempDataOut[i] = tempDataIn[i];
	}	
}